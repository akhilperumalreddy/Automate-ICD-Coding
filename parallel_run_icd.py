import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import gc
from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup, set_seed
from sklearn.metrics import f1_score, accuracy_score
import accelerate
from accelerate import Accelerator, DistributedDataParallelKwargs
from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader
from tqdm import tqdm

class CustomDataset(Dataset):
    def __init__(self, inputs, outputs):
        self.inputs = inputs
        self.outputs = outputs
        
    def __len__(self):
        return len(self.inputs)
    
    def __getitem__(self, idx):
        input = self.inputs[idx]
        output = self.outputs[idx]
        return input, output


###key hyperparameters
max_length_tokenization = 3072 ## max length of token IDs (for each word)
padding_tokentization = True ## whether to pad the token IDs
# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])
print(accelerator.state)
set_seed(143)
# **0. Load Input data, split the text to sentences for BERT & Tokenize to IDs, onehot encoding of lables**


##get all labels (top 50 as of now)
all_labels = pd.read_csv('../data/parallel/top_x_codes.txt', header=None)
all_labels.columns = ["ICD9_CODE"]
all_labels = all_labels['ICD9_CODE'].tolist()

def split_text_to_sentences(text, tokenizer,max_length = 128, padding = False, token_length = 3072 ):
    text = text.replace('"', '') ## to remove quotes at the beginning and end of the text
    #text = text.split(' ') ## to split the text into words
    encoded_dict = tokenizer(text, padding=padding, truncation=True, max_length=token_length, add_special_tokens=True)
    input_ids = encoded_dict['input_ids']
    sentences = []
    attention_masks = []
    token_type_ids = []
    for i in range(0, len(input_ids), max_length):
        sentences.append(input_ids[i:i+max_length]) ## last sentence may be less than max_length
        attention_masks.append(encoded_dict['attention_mask'][i:i+max_length])
        token_type_ids.append(encoded_dict['token_type_ids'][i:i+max_length])
        if len(sentences[-1]) < max_length: ## if last sentence is less than max_length, pad it with [PAD]
            # tl =len(sentences[-1][0]) ## length of token IDs generated by tokenizer
            # sentences[-1] = sentences[-1] + [[0]*tl]*(max_length - len(sentences[-1])) ## pad with 0
            # attention_masks[-1] = attention_masks[-1] + [[0]*tl]*(max_length - len(attention_masks[-1])) ## pad with 0
            # token_type_ids[-1] = token_type_ids[-1] + [[0]*tl]*(max_length - len(token_type_ids[-1])) ## pad with 0

            sentences[-1] = sentences[-1] + [0]*(max_length - len(sentences[-1])) ## pad with 0
            attention_masks[-1] = attention_masks[-1] + [0]*(max_length - len(attention_masks[-1])) ## pad with 0
            token_type_ids[-1] = token_type_ids[-1] + [0]*(max_length - len(token_type_ids[-1])) ## pad with 0

    return [sentences, attention_masks, token_type_ids]

def labels_to_one_hot(labels, all_labels):
    one_hot_labels = []
    for label in labels:
        one_hot = [0]*len(all_labels)
        for code in label:
            one_hot[all_labels.index(code)] = 1
        one_hot_labels.append(one_hot)
    return one_hot_labels
    



def data_pull(df, all_labels, padding_tokentization, max_length_tokenization ):
    tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_Discharge_Summary_BERT", use_fast=True)
    notes = df['TEXT'].apply(lambda x: split_text_to_sentences(x, tokenizer, 128,padding_tokentization, max_length_tokenization)) #list of list of sentences
    labels = df['ICD9_CODE'].apply(lambda x: x.split(';')) ##list of ICD9 codes
    notes = notes.values.tolist() ## convert to list
    labels = labels.values.tolist() ## convert to list
    one_hot_labels = labels_to_one_hot(labels, all_labels)

    return notes, one_hot_labels

def batch_collator(inputs):
    batch = dict()
    max_num_sentences = max([len(x[0][0]) for x in inputs])

    batch_sentences, batch_attention_masks, batch_token_type_ids, batch_labels = [], [], [], []   
    for j in range(len(inputs)):
        num_sentences = len(inputs[j][0][0])
        batch_sentences.append(inputs[j][0][0] + [[0]*128]*(max_num_sentences - num_sentences))
        batch_attention_masks.append(inputs[j][0][1] + [[0]*128]*(max_num_sentences - num_sentences))
        batch_token_type_ids.append(inputs[j][0][2] + [[0]*128]*(max_num_sentences - num_sentences))
        batch_labels.append(inputs[j][1])

    batch["input_ids"] = torch.tensor(batch_sentences)
    batch["attention_mask"] = torch.tensor(batch_attention_masks)
    batch["token_type_ids"] = torch.tensor(batch_token_type_ids)
    batch["labels"] = torch.tensor(batch_labels)
    
    return batch

train_df = pd.read_csv('../data/parallel/train_ds_notes.csv')
train_notes, train_labels = data_pull(train_df[:100], all_labels,padding_tokentization, max_length_tokenization)

train_dataset = CustomDataset(train_notes,train_labels)
train_dataloader = DataLoader(train_dataset,shuffle = True, collate_fn=batch_collator, batch_size=1)
dev_df = pd.read_csv('../data/parallel/dev_ds_notes.csv')
#TAKE one in 6 rows
dev_df = dev_df[:5000]
dev_df = dev_df.iloc[::6, :]
dev_notes, dev_labels = data_pull(dev_df, all_labels,padding_tokentization, max_length_tokenization)
dev_dataset = CustomDataset(dev_notes,dev_labels)
dev_dataloader = DataLoader(dev_dataset, collate_fn=batch_collator, batch_size=2)

##clearning memory

gc.collect()
del train_df
del dev_df


# **1. Main Model Definition (BERT + LabelAttention + Loss)**


class ICD9_Detection(nn.Module):
    def __init__(self, num_labels):
        super(ICD9_Detection, self).__init__()
        self.bert = AutoModel.from_pretrained("emilyalsentzer/Bio_Discharge_Summary_BERT")
        self.dropout = nn.Dropout(0.1)
        self.linear_z = nn.Linear(768, 768)
        self.linear_a = nn.Linear(768, num_labels)
        self.linear_o = nn.Linear(768, num_labels)
        self.num_labels = num_labels

    def forward(self, input_ids, attention_mask, token_type_ids):
        outputs = self.bert(input_ids.view(-1, 128), attention_mask=attention_mask.view(-1, 128), token_type_ids=token_type_ids.view(-1, 128))
        last_hidden_state = outputs[0].view(input_ids.shape[0],input_ids.shape[1]*input_ids.shape[2], 768) ##shape: (b, s*c, 768)
        z = torch.tanh(self.linear_z(last_hidden_state)) ##shape: (b, s*c, 768)
        a = torch.softmax(self.linear_a(z), dim=1).transpose(1,2) ##shape: (b, num_labels, s*c) weights for each label
        d = torch.matmul(a, last_hidden_state) ##shape: (b, num_labels, 768) weighted sum for each label (check matmul once again)
        logits = self.linear_o.weight.mul(d).sum(dim=2) ##shape: (num_labels) logits for each label
        return logits
    
    def loss(self, logits, labels):
        loss = nn.BCEWithLogitsLoss()
        return loss(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))


def batch_convertor(training_data, training_labels, batch_size = 32):##add padding when # sentences differes across batches
    all_batches_sentences, all_batches_attention, all_batches_tokentype = [], [], []
    all_batches_labels = []
    for i in range(0, len(training_data), batch_size):
        batch = training_data[i:i+batch_size]
        max_num_sentences = max([len(x[0]) for x in batch])
        batch_sentences, batch_attention_masks, batch_token_type_ids = [], [], []
        for j in range(len(batch)):
            num_sentences = len(batch[j][0])
            batch_sentences.append(batch[j][0] + [[0]*128]*(max_num_sentences - num_sentences))
            batch_attention_masks.append(batch[j][1] + [[0]*128]*(max_num_sentences - num_sentences))
            batch_token_type_ids.append(batch[j][2] + [[0]*128]*(max_num_sentences - num_sentences))

        all_batches_sentences.append(torch.tensor(batch_sentences))
        all_batches_attention.append(torch.tensor(batch_attention_masks))
        all_batches_tokentype.append(torch.tensor(batch_token_type_ids))
        all_batches_labels.append(torch.tensor(training_labels[i:i+batch_size]))
    
    return [all_batches_sentences, all_batches_attention,all_batches_tokentype], all_batches_labels




## training by taking one example at a time
def train(model,train_dataloader, dev_dataloader, num_epochs, optimizer, device, model_save_path = None, threshold=0.2):
    print('Training started')
    for epoch in range(num_epochs):
        epoch_loss = []
        model.train()
        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):
            note = batch['input_ids']
            note = batch['input_ids']
            attention_mask = batch['attention_mask']
            token_type_ids = batch['token_type_ids']
            label = batch['labels']
            logits = model(note, attention_mask, token_type_ids)
            loss = model.module.loss(logits, label.float())
            accelerator.backward(loss)
            optimizer.step()
            optimizer.zero_grad()
            torch.cuda.empty_cache()
            del note, attention_mask, token_type_ids, logits
            if step%5000 == 0 and accelerator.is_local_main_process:
                model.eval()
                dev_micro_f1, dev_macro_f1 = evaluate_model(model, dev_dataloader, device)
                print('Epoch: {}, #Batches: {}, Loss: {}, Dev Micro F1: {}, Dev Macro F1: {}'.format(epoch, step, loss.item(), dev_micro_f1, dev_macro_f1))

            epoch_loss.append(loss.item())
            del loss
        print('Epoch: {}, Loss: {}'.format(epoch, np.mean(epoch_loss)))
        if accelerator.is_local_main_process:
            ###save model
            model_save_path = model_save_path + str(epoch) + '.pt'
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            torch.save(unwrapped_model.state_dict(), model_save_path)
            print('Model saved to {}'.format(model_save_path))

    return model

def evaluate_model (model, dev_dataloader, device=torch.device("cpu"), threshold=0.2):
    #model.eval()
    with torch.no_grad():
        predictions = []
        actual = []
        for step,batch in enumerate(dev_dataloader):
            note = batch['input_ids']
            attention_mask = batch['attention_mask']
            token_type_ids = batch['token_type_ids']
            logits = model(note, attention_mask, token_type_ids)
            actual_batch = batch['labels']
            for ll in range(len(logits)):#for each example in the batch
                logits_i = torch.sigmoid(logits[ll])
                logits_i = [1 if x>threshold else 0 for x in logits_i]
                predictions.append(logits_i) ## appending for each example in the batch/dataset
                actual.append(actual_batch[ll])
            del logits, note, attention_mask,token_type_ids
    

    actual = [ x.tolist() for x in actual ]
    ##f1_score calc
    micro_f1 = f1_score(torch.tensor(actual).numpy(), torch.tensor(predictions).numpy(), average='micro')
    macro_f1 = f1_score(torch.tensor(actual).numpy(), torch.tensor(predictions).numpy(), average='macro')

    #print('Micro F1: {}, Macro F1: {}'.format(micro_f1, macro_f1))

    return micro_f1, macro_f1


##train the model
if torch.cuda.is_available():
    device = torch.device("cuda")
    print('GPU device is available')
else:
    device = torch.device("cpu")
    print('GPU device is not available, using CPU instead')
    
model = ICD9_Detection(len(all_labels))

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
model_save_path = 'parallel_saved_models/SH_ICD9_detection_model_epoch_'


# Prepare everything with our `accelerator`.
model, optimizer = accelerator.prepare( model, optimizer )
train_dataloader = accelerator.prepare(train_dataloader)
dev_dataloader = accelerator.prepare(dev_dataloader)

model = train(model, train_dataloader, dev_dataloader, num_epochs =5, optimizer= optimizer, device = device , model_save_path=model_save_path, threshold=0.15)

evaluate_model(model, dev_dataloader, device, threshold=0.2)

